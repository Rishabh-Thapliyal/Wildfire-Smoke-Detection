{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28257125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0a0+40ec155e58.nv24.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 19.18it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 93.38 MiB is free. Process 1524547 has 18.21 GiB memory in use. Including non-PyTorch memory, this process has 13.43 GiB memory in use. Of the allocated memory 13.00 GiB is allocated by PyTorch, and 142.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(model_dict\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlavaOnevisionForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_dict[model_name])\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_grid_on_image\u001b[39m(image, grid_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m)):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3698\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3694\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3695\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3696\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3697\u001b[0m         )\n\u001b[0;32m-> 3698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 93.38 MiB is free. Process 1524547 has 18.21 GiB memory in use. Including non-PyTorch memory, this process has 13.43 GiB memory in use. Of the allocated memory 13.00 GiB is allocated by PyTorch, and 142.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchprofile import profile_macs\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "        \n",
    "# Ignore all DeprecationWarnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.sys.path.append(\"/expanse/lustre/projects/ddp464/mhnguyen/data\")\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n",
    "\n",
    "model_dict = { \"ov_chat\" :\"llava-hf/llava-onevision-qwen2-7b-ov-chat-hf\",\n",
    "\"ov\" : \"llava-hf/llava-onevision-qwen2-7b-ov-hf\",\n",
    "\"si\" : \"llava-hf/llava-onevision-qwen2-7b-si-hf\"\n",
    "}\n",
    "\n",
    "# =========== select the model =========\n",
    "\n",
    "model_name = list(model_dict.keys())[0]\n",
    "\n",
    "# ========================================\n",
    "\n",
    "model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "    model_dict[model_name], \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    ").to(device = 'cuda')\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_dict[model_name])\n",
    "\n",
    "def draw_grid_on_image(image, grid_size=(3, 3)):\n",
    "    \"\"\"\n",
    "    Draws a grid on the image.\n",
    "    \n",
    "    :param image: PIL Image object\n",
    "    :param grid_size: Tuple (rows, cols) indicating the grid dimensions\n",
    "    :return: PIL Image object with grid lines drawn\n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    width, height = image.size\n",
    "    grid_width = width // grid_size[1]\n",
    "    grid_height = height // grid_size[0]\n",
    "    \n",
    "    # Draw vertical lines\n",
    "    for i in range(1, grid_size[1]):\n",
    "        x = i * grid_width\n",
    "        draw.line((x, 0, x, height), fill=\"red\", width=2)\n",
    "    \n",
    "    # Draw horizontal lines\n",
    "    for i in range(1, grid_size[0]):\n",
    "        y = i * grid_height\n",
    "        draw.line((0, y, width, y), fill=\"red\", width=2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def parse_result(output_text):\n",
    "    \"\"\"\n",
    "    Parses the model's output to extract \"Yes\" or \"No\".\n",
    "    \n",
    "    :param output_text: The model's output text\n",
    "    :return: \"Yes\" or \"No\" based on the output\n",
    "    \"\"\"\n",
    "    # Use regex to find \"Yes\" or \"No\" in the output\n",
    "    match = re.search(r'\\b(Yes|No)\\b', output_text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return \"No\"  # Default to \"No\" if no match is found\n",
    "\n",
    "def parse_assistant_response(output_text):\n",
    "    \"\"\"\n",
    "    Parses the model's output to extract only the assistant's response.\n",
    "    \n",
    "    :param output_text: The model's output text\n",
    "    :return: The assistant's response (starting from \"Result: Yes\" or \"Result: No\")\n",
    "    \"\"\"\n",
    "    # Find the start of the assistant's response\n",
    "    assistant_start = output_text.find(\"assistant\")\n",
    "    if assistant_start == -1:\n",
    "        return output_text  # If \"assistant\" is not found, return the entire output\n",
    "    \n",
    "    # Extract the assistant's response\n",
    "    assistant_response = output_text[assistant_start:].split(\"assistant\")[-1].strip()\n",
    "    return assistant_response\n",
    "\n",
    "with open('train_fires_final.txt', 'r') as file:\n",
    "    train_fire_names = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# print(fire_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827fc687",
   "metadata": {},
   "outputs": [],
   "source": [
    "palisades_image_dict = {\n",
    "    0: \"http://legacy-www.hpwren.ucsd.edu/FIgLib/HPWREN-FIgLib-Data/20250107_PalisadesFire_69bravo-e-mobo-c/1736274241_%2B00000.jpg\",\n",
    "    1: \"http://legacy-www.hpwren.ucsd.edu/FIgLib/HPWREN-FIgLib-Data/20250107_PalisadesFire_69bravo-e-mobo-c/1736274301_%2B00060.jpg\",\n",
    "    2: \"http://legacy-www.hpwren.ucsd.edu/FIgLib/HPWREN-FIgLib-Data/20250107_PalisadesFire_69bravo-e-mobo-c/1736274362_%2B00121.jpg\",\n",
    "    3: \"http://legacy-www.hpwren.ucsd.edu/FIgLib/HPWREN-FIgLib-Data/20250107_PalisadesFire_69bravo-e-mobo-c/1736274422_%2B00181.jpg\",\n",
    "    6: \"http://legacy-www.hpwren.ucsd.edu/FIgLib/HPWREN-FIgLib-Data/20250107_PalisadesFire_69bravo-e-mobo-c/1736274601_%2B00360.jpg\",\n",
    "    12: \"http://legacy-www.hpwren.ucsd.edu/FIgLib/HPWREN-FIgLib-Data/20250107_PalisadesFire_69bravo-e-mobo-c/1736274961_%2B00720.jpg\",\n",
    "    24: \"http://legacy-www.hpwren.ucsd.edu/FIgLib/HPWREN-FIgLib-Data/20250107_PalisadesFire_69bravo-e-mobo-c/1736275681_%2B01440.jpg\",   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ec7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load example images (adjust paths to your few-shot images)\n",
    "example_image_1 = Image.open(requests.get(palisades_image_dict[24], stream=True).raw)  # Image with smoke\n",
    "example_image_2 = Image.open(requests.get(palisades_image_dict[0], stream=True).raw)  # Image without smoke\n",
    "\n",
    "# Define few-shot examples (with actual images)\n",
    "few_shot_examples = [\n",
    "    # Example 1: Smoke present\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Is there a smoke plume in this image?\"},\n",
    "            {\"type\": \"image\", \"image\": example_image_1}  # Embed image\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Result: Yes. Smoke detected in Row 2, Column 3. Confidence: 9/10.\"\n",
    "    },\n",
    "    \n",
    "    # Example 2: No smoke\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Is there smoke in this image?\"},\n",
    "            {\"type\": \"image\", \"image\": example_image_2}  # Embed image\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Result: No. No visible smoke plume. Confidence: 9/10.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Current query (your actual input image)\n",
    "current_image = Image.open(requests.get(palisades_image_dict[12], stream=True).raw)  # Replace with your image\n",
    "current_query = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": f\"\"\"\n",
    "        Is there a smoke plume in this image?\n",
    "        Answer strictly with \"Result: Yes\" or \"Result: No\" first.\n",
    "        If yes, specify grid location (Row X, Column Y).\n",
    "        Confidence (0-10).\n",
    "        \"\"\"},\n",
    "        {\"type\": \"image\", \"image\": current_image}  # Your input image\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Combine few-shot examples + current query\n",
    "conversation = few_shot_examples + [current_query]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39181992",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcurrent_query\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'current_query' is not defined"
     ]
    }
   ],
   "source": [
    "current_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c392e6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user \n",
      "Is there a smoke plume in this image?assistant user \n",
      "Is there smoke in this image?assistant user \n",
      "\n",
      "        Is there a smoke plume in this image?\n",
      "        Answer strictly with \"Result: Yes\" or \"Result: No\" first.\n",
      "        If yes, specify grid location (Row X, Column Y).\n",
      "        Confidence (0-10).\n",
      "        assistant\n",
      "Result: Yes\n"
     ]
    }
   ],
   "source": [
    "# Process all images and text\n",
    "inputs = processor(\n",
    "    text=processor.apply_chat_template(conversation, add_generation_prompt=True),\n",
    "    images=[item[\"content\"][1][\"image\"] for item in conversation if item[\"role\"] == \"user\"],  # Extract all images\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to(\"cuda\", torch.float32)\n",
    "    \n",
    "# Generate response\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=False,\n",
    "    pad_token_id=model.config.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode output\n",
    "response = processor.decode(output[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37797ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
